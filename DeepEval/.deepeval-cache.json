{"test_cases_lookup_map": {"{\"actual_output\": \"We offer a 30-day full refund at no extra cost.\", \"context\": null, \"expected_output\": null, \"hyperparameters\": null, \"input\": \"What if these shoes don't fit?\", \"model\": null, \"retrieval_context\": [\"All customers are eligible for a 30 day full refund at no extra cost.\"], \"user_prompt_template\": null}": {"cached_metrics_data": [{"metric_metadata": {"metric": "Answer Relevancy", "score": 1.0, "threshold": 0.7, "success": true, "reason": "The score is 1.00 because the response perfectly addresses the concern without any irrelevant information. Keep up the good work!", "strictMode": false, "evaluationModel": "gpt-4-0125-preview"}, "metric_configuration": {"threshold": 0.7, "evaluation_model": "gpt-4-0125-preview", "strict_mode": false, "include_reason": true}}, {"metric_metadata": {"metric": "GEval (Coherence)", "score": 0.3617625126545695, "threshold": 0.5, "success": false, "reason": "The statement is clear and straightforward, indicating good clarity. However, it lacks context, progression, and detailed organization that could enhance comprehension and coherence, limiting its effectiveness in maintaining a consistent theme or argument.", "strictMode": false, "evaluationModel": "gpt-4-0125-preview"}, "metric_configuration": {"threshold": 0.5, "evaluation_model": "gpt-4-0125-preview", "strict_mode": false, "criteria": "Coherence - determine if the actual output is logical, has flow, and is easy to understand and follow.", "include_reason": false, "evaluation_steps": ["Read through the actual output to assess if the content logically progresses from one point to another, ensuring a natural flow.", "Evaluate the clarity of the actual output by determining if the information presented is easy to understand without requiring further explanation or context.", "Compare the actual outputs to each other to identify which ones maintain a consistent theme or argument throughout, indicating higher coherence.", "Consider the organization of information in the actual output and whether it aids in the reader's comprehension and retention of the material."], "evaluation_params": ["actual_output"]}}, {"metric_metadata": {"metric": "Length", "score": 0.0, "threshold": 10.0, "success": true, "strictMode": false}, "metric_configuration": {"threshold": 10.0, "strict_mode": false, "include_reason": false}}]}}}